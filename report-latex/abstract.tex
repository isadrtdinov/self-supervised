\section*{Аннотация} 
\addcontentsline{toc}{section}{Аннотация}

В последние годы парадигма self-supervised learning набирает популярность в задачах компьютерного зрения. Она позволяет предобучать искусственные нейронные сети на неразмеченных данных. При этом качество на контрольных задачах получается лучше, чем при предобучении на классификацию набора данных ImageNet. В данной работе мы проводим сравнительный анализ парадигмы self-supervised learning, обучения с учителем и обучения со случайной разметкой и приходим к выводу, что в контексте всех трех постановок существуют простые и сложные для выучивания нейронной сетью объекты. Также мы показываем, что динамика обучения в парадигме self-supervised learning похожа на динамику обучения со случайной разметкой. 

\section*{Abstract}

Self-supervised learning in computer vision tasks has been gaining popularity recently. This paradigm is used to pre-train artificial neural networks on unlabelled data. Modern self-supervised methods considerably outperform ImageNet pre-training. In this work, we compare self-supervised learning, supervised learning, and training on random labels. We show that for each setup, there exist easy and hard examples to be memorized by the neural network. We also demonstrate that training dynamics of self-supervised learning and random labels training are similar.

\bigskip
\noindent
\textbf{Список ключевых слов}: self-supervised learning, SimCLR, динамика обучения, случайная разметка, эффект запоминания
